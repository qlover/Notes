# Scrapy


	building 'twisted.test.raiser' extension
	error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools

: https://www.jb51.net/article/125081.htm


`scrapy startproject [项目名]`

`scrapy genspider [爬虫名] ['http://' 爬取的域]`

`scrapy crawl [爬虫名]` 	# 执行爬虫



# Spider #

是最基本的类，所有编写的爬虫必须继承这个类

+ name
	name 是 spider 类最基本的属性，也是必须的属性
+ allowed_domains
	包含了 spider 允许爬取的域名的列表，可选
+ start_urls
	初始 url 元组/列表，当没有特定的 URL 时， spider 将从该列表中进行爬取
+ start_requests()
	该方法必须返回一个可迭代对象，该对象包含了 spider 用于爬取的第一个 Request

+ paser()
	当请求 url 返回网页没有指定回调函数时，默认的 Request 对象回调函数，用来处理网页返回的 response, 以及生成 item 或者 Request 对象
+ log()
	使用 scrapy.log.msg() 方法记录 log 时





### 腾讯招聘

[腾讯招聘](https://hr.tencent.com/position.php)

职位名	
positionName
职位链接	
positionLink
职位类型	
positionType
招聘人数	
peopelNumber
工作地点	
workLocation
发布时间	
publishTime




>> ModuleNotFoundError: No module named 'win32api'
[](https://sourceforge.net/projects/pywin32/files/pywin32/Build%20221/)
安装相同版本的 pywin32 

>> Object of type 'bytes' is not JSON serializable
[](https://blog.csdn.net/z564359805/article/details/80599126)
[](https://blog.csdn.net/bear_sun/article/details/79397155)















https://blog.csdn.net/daybreak1209/article/details/52425308









第二章 搜索引擎

搜索引擎（Search Engine）是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务，将用户检索相关的信息展示给用户的系统。搜索引擎包括全文索引、目录索引、元搜索引擎、垂直搜索引擎、集合式搜索引擎、门户搜索引擎与免费链接列表等。

搜索引擎的性能主要取决于：索引数据库的容量存放内容更新和搜索速度是否易用等搜索引擎是以传统信息检索技术为基础利用其索引模型匹配策略等方面的技术成果并针对Web 资源的特点发展起来的信息检索技术涉及多领域的理论和技术数据库信息检索人工智能自然语言处理统计数据分析数据挖掘计算机网络分布式处理等本文以工作方式对搜索引擎进行分类介绍搜索引擎各组成部分的相关研究和关键技术搜索器策略检索策略搜索结果处理信息检索Agent 多媒体搜索引擎等并对未来搜索引擎的主要发展方向进行了展望
2.1 搜索引擎的分类
按照信息搜集方法 服务提供方式和系统结构的不同搜索引擎系统可以分为不同的类别图2-1 所示搜索引擎系统按其工作机制可以分为以下类别：

1.	机器人搜索
2.	目录式搜索
3.	元搜索
4.	信息检索



图 2-1搜索引擎工作机制分类


2.2 搜索引擎的技术
一般搜索引擎由搜索器 分析器索引器检索器和用户接口等5 个部分组成如图2-2所示：


图 2-2搜索引擎组成结构

2.2.1 搜索器
由于 Web 信息的大容量分布性和动态性保持全面而又最新的资料收集是影响搜索引擎性能的重要方面搜索器是一个机器人程序Robot (也称为Spider crawler 或wander)自动地在互联网中搜集信息下载到本地文档库为提高其工作效率常使用分布式并行计算技术主要有2 个方面的工作重点
1.搜索策略包括优先搜索重要网页的策略；基于超链接的结构分析[3]和机器学习技术；对特殊题目或类型网页的搜索与Web 页面URL 有关的启发式规则研究；根据页面更新历史优化网页集合保持最新页面集的技术；在网站地图中随机走步进行网页取样搜索的研究[7] 搜索隐藏网页的研究[8]等
2 设计高性能系统结构以支持每秒下载大量网页同时确保系统具有较好的管理性和健壮性
2.2.2 分析器
分析器对本地文档库进行分析以便用于索引 文档分析技术包括分词过滤和转换等
2.2.3 索引器
索引器的功能是理解搜索器所搜索的信息从中抽取出索引项将文档表示为一种便于检索的方式并存储在索引数据库中生成文档库的索引表
索引项有客观索引项和内容索引项两种、客观项与文档的语意内容无关如作者名URL 更新时间长度链接流行度Link Popularity 等内容索引项可分为单索引项和多索引项或称短语索引项用来反映文档内容如关键词及其权重短语单字等
2.2.4检索器
检索器的功能是根据用户的查询在索引库中找出相关文档进行文档与查询的相关度评价返回相关度符合某一阈值的文档集合其检索方法有以下几种：
1 基于关键词的检索
2 基于概念的检索
3 基于内容的检索

2.2.5 用户接口
用户接口的作用是为用户提供可视化的查询输入和结果输出界面提供用户相关性反馈机制。在输出界面中搜索引擎将检索结果展现为一个线性的文档列表其中包含了文档的标题摘要所在URL 等信息用户需要逐个浏览以寻找出所需的文档。

2.3网页搜索分析
2.3.1拓扑分析算法
基于网页之间的链接，通过已知的网页或数据，来对与其有直接或间接链接关系的对象（可以是网页或网站等）作出评价的算法。又分为网页粒度、网站粒度和网页块粒度这三种。


1 网页(Webpage)粒度的分析算法
PageRank和HITS算法是最常见的链接分析算法，两者都是通过对网页间链接度的递归和规范化计算，得到每个网页的重要度评价。PageRank算法虽然考虑了用户访问行为的随机性和Sink网页的存在，但忽略了绝大多数用户访问时带有目的性，即网页和链接与查询主题的相关性。针对这个问题，HITS算法提出了两个关键的概念：权威型网页（authority）和中心型网页（hub）。
基于链接的抓取的问题是相关页面主题团之间的隧道现象，即很多在抓取路径上偏离主题的网页也指向目标网页，局部评价策略中断了在当前路径上的抓取行为。文献[21]提出了一种基于反向链接（BackLink）的分层式上下文模型（Context Model），用于描述指向目标网页一定物理跳数半径内的网页拓扑图的中心Layer0为目标网页，将网页依据指向目标网页的物理跳数进行层次划分，从外层网页指向内层网页的链接称为反向链接。
2 网站粒度的分析算法
网站粒度的资源发现和管理策略也比网页粒度的更简单有效。网站粒度的爬虫抓取的关键之处在于站点的划分和站点等级(SiteRank)的计算。SiteRank的计算方法与PageRank类似，但是需要对网站之间的链接作一定程度抽象，并在一定的模型下计算链接的权重。
网站划分情况分为按域名划分和按IP地址划分两种。文献[18]讨论了在分布式情况下，通过对同一个域名下不同主机、服务器的IP地址进行站点划分，构造站点图，利用类似PageRank的方法评价SiteRank。同时，根据不同文件在各个站点上的分布情况，构造文档图，结合SiteRank分布式计算得到DocRank。文献[18]证明，利用分布式的SiteRank计算，不仅大大降低了单机站点的算法代价，而且克服了单独站点对整个网络覆盖率有限的缺点。附带的一个优点是，常见PageRank 造假难以对SiteRank进行欺骗。
3 网页块粒度的分析算法
在一个页面中，往往含有多个指向其他页面的链接，这些链接中只有一部分是指向主题相关网页的，或根据网页的链接锚文本表明其具有较高重要性。但是，在PageRank和HITS算法中，没有对这些链接作区分，因此常常给网页分析带来广告等噪声链接的干扰。在网页块级别(Block?level)进行链接分析的算法的基本思想是通过VIPS网页分割算法将网页分为不同的网页块(page block)，然后对这些网页块建立page?to?block和block?to?page的链接矩阵，?分别记为Z和X。于是，在page?to?page图上的网页块级别的PageRank为?W?p=X×Z；?在block?to?block图上的BlockRank为?W?b=Z×X。已经有人实现了块级别的PageRank和HITS算法，并通过实验证明，效率和准确率都比传统的对应算法要好。
2.3.2网页内容分析算法
基于网页内容的分析算法指的是利用网页内容（文本、数据等资源）特征进行的网页评价。网页的内容从原来的以超文本为主，发展到后来动态页面（或称为Hidden Web）数据为主，后者的数据量约为直接可见页面数据（PIW，Publicly Indexable Web）的400~500倍。另一方面，多媒体数据、Web Service等各种网络资源形式也日益丰富。因此，基于网页内容的分析算法也从原来的较为单纯的文本检索方法，发展为涵盖网页数据抽取、机器学习、数据挖掘、语义理解等多种方法的综合应用。本节根据网页数据形式的不同，将基于网页内容的分析算法，归纳以下三类：第一种针对以文本和超链接为主的无结构或结构很简单的网页；第二种针对从结构化的数据源（如RDBMS）动态生成的页面，其数据不能直接批量访问；第三种针对的数据界于第一和第二类数据之间，具有较好的结构，显示遵循一定模式或风格，且可以直接访问。
基于文本的网页分析算法
1) 纯文本分类与聚类算法
很大程度上借用了文本检索的技术。文本分析算法可以快速有效的对网页进行分类和聚类，但是由于忽略了网页间和网页内部的结构信息，很少单独使用。
2) 超文本分类和聚类算法
根据网页链接网页的相关类型对网页进行分类，依靠相关联的网页推测该网页的类型。



2.4 统一资源定位器
统一资源定位器(URL) 借用搜索引擎可以从互联网上得到的资源,也是对互联网上得的资源访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。它最初是由蒂姆·伯纳斯·李发明用来作为万维网的地址。现在它已经被万维网联盟编制为互联网标准RFC1738了。
2.4.1 结构
基本URL包含模式（或称协议）、服务器名称（或IP地址）、路径和文件名，如“协议://授权/路径?查询”。完整的、带有授权部分的普通统一资源标志符语法看上去如下：
协议://用户名:密码@子域名.域名.顶级域名:端口号/目录/文件名.文件后缀?参数=值#标志
其中最常用到的协议则是超文本传输协议（Hypertext Transfer Protocol， HTTP），经过 ssl 套接字协议的封装上，则为 HTTPS。
并且统一资源定位符一般是分大小写的，不过服务器管理员可以确定在回复询问时大小写是否被区分。
2.4.2 分类
互联网上的 URL 主要分为两大类：
1.绝对 URL：绝对URL（absolute URL）显示文件的完整路径，这意味着绝对URL本身所在的位置与被引用的实际文件的位置无关
2.相对 URL：相对URL（relative URL）以包含URL本身的文件夹的位置为参考点，描述目标文件夹的位置。如果目标文件与当前页面（也就是包含URL的页面）在同一个目录，那么这个文件的相对URL仅仅是文件名和扩展名，如果目标文件在当前目录的子目录中，那么它的相对URL是子目录名，后面是斜杠，然后是目标文件的文件名和扩展名。
第三章 网络爬虫
网络爬虫(Web Crawler)，又称为网络蜘蛛(Web Spider)或Web信息采集器，是一个自动下载网页的计算机程序或自动化脚本，是搜索引擎的重要组成部分。
网络爬虫通常从一个称为种子集的URL集合开始运行，它首先将这些URL全部放入到一个有序的待爬行队列里，按照一定的顺序从中取出URL并下载所指向的页面，分析贞面内容，提取新的URL并存入待爬行URL队列中，如此
重复I二面的过程，直到URL队列为空或满足某个爬行终止条件，从而遍历Wel／1I

网络爬虫（Web crawler），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，它们被广泛用于互联网搜索引擎或其他类似网站，可以自动采集所有其能够访问到的页面内容，以获取或更新这些网站的内容和检索方式。从功能上来讲，爬虫一般分为数据采集，处理，储存三个部分。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重复上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索；对于聚焦爬虫来说，这一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。

3.1网络爬虫的分类
3.1.1通用网络爬虫
通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子URL 扩充到整个Web，主要为门户站点搜索引擎和大型Web 服务提供商采集数据。由于商业原因，它们的技术细节很少公布出来。这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。虽然存在一定缺陷，通用网络爬虫适用于为搜索引擎搜索
广泛的主题，有较强的应用价值。
通用网络爬虫的结构大致可以分为页面爬行模块、页面分析模块、链接过滤
模块、页面数据库、URL 队列、初始URL 集合几个部分，其体系结构如图所示：


图3-1-1 通用网络爬虫体系结构


爬虫为提高工作效率，通用网络爬虫会采取一定的爬行策略。常用的爬行策略有：深度优先策略、广度优先策略
1) 深度优先策略：其基本方法是按照深度由低到高的顺序，依次访问下一级
网页链接，直到不能再深入为止。爬虫在完成一个爬行分支后返回到上一链接节
点进一步搜索其它链接。当所有链接遍历完后，爬行任务结束。这种策略比较适合垂直搜索或站内搜索， 但爬行页面内容层次较深的站点时会造成资源的巨大浪费；
2) 广度优先策略：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需较长时间才能爬行到目录层次较深的页面。典型的通用爬虫有Google Crawler、Mercator。Google Crawler[6]是一个分布式的基于整个Web 的爬虫，采用异步I/O 而不是多线程来实现并行化。它有一个专门的URL Server 进程负责为多个爬虫节点维护URL 队列。Google Crawler 还使用了许多算法优化系统性能，最著名的就是PageRank 算法。




3.1.2聚焦网络爬虫
聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler），是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫[8]。 和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求 [1]  。
	聚焦网络爬虫和通用网络爬虫相比，增加了链接评价模块以及内容评
价模块，其系统结构图 2-1-2 所示：



图 3-1-2聚焦网络爬虫体系结构

聚焦网络爬虫和通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚焦爬虫爬行策略实现的关键是评价页面内容和链接的重要性，不同的方法计算出的重要性不同，由此导致链接的访问顺序也不同 [1]  。

1) 基于内容评价的爬行策略：DeBra将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关 度 的 高 低 。 Herseovic对 Fish Search 算 法 进 行 了 改 进 ，提 出 了 Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小。

2) 基于链接结构评价的爬行策略 ：Web 页面作为一种半结构化文档，包含很多结构信息，可用来评价链接重要性。 PageRank 算法最初用于搜索引擎信息检索中对查询结果进行排序，也可用于评价链接重要性，具体做法就是每次选择 PageRank 值较大页面中的链接来访问。 另一个利用 Web结构评价链接价值的方法是 HITS 方法，它通过计算每个已访问页面的 Authority 权重和 Hub 权重，并以此决定链接的访问顺序。


3) 基于增强学习的爬行策略：Rennie 和 McCallum 将增强学习引入聚焦爬虫，利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序。

4) 基于语境图的爬行策略：Diligenti 等人提出了一种通过建立语境图（Context Graphs）学习网页之间的相关度，训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离越近的页面中的链接优先访问。印度理工大学（IIT）和 IBM 研究中心的研究人员开发了一个典型的聚焦网络爬虫。 该爬虫对主题的定义既不是采用关键词也不是加权矢量，而是一组具有相同主题的网页。 它包含两个重要模块：一个是分类器，用来计算所爬行的页面与主题的相关度，确定是否与主题相关；另一个是净化器，用来识别通过较少链接连接到大量相关页面的中心页面。

3.1.3增量式网络爬虫
增量式网络爬虫（Incremental Web Crawler）[16]是指对已下载网页采取增量
式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上
保证所爬行的页面是尽可能新的页面。和周期性爬行和刷新页面的网络爬虫相
比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面，并不重新下
载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小
时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。

图3-1-3 增量式爬虫体系结构

增量式爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面的质量。为实现第一个目标，增量
式爬虫需要通过重新访问网页来更新本地页面集中页面内容，常用的方法有：
1) 统一更新法[17]：爬虫以相同的频率访问所有网页，不考虑网页的改变频率；
2) 个体更新法[17]：爬虫根据个体网页的改变频率来重新访问各页面；
3) 基于分类的更新法[18]：爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问
这两类网页。

3.1.4 Deep Web 爬虫
Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称Invisible Web Pages 或Hidden Web）。表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的Web 页面。Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的Web 页面。例如那些用户注册后内容才可见的网页
就属于Deep Web。2000 年Bright Planet 指出[22]：Deep Web 中可访问信息容量是Surface Web 的几百倍，是互联网上最大、发展最快的新型信息资源。

Deep Web 爬虫体系结构如图3-1-4 所示， 包含六个基本功能模块[24]（爬行控制器、解析器、表单分析器、表单处理器、响应分析器、LVS 控制器）和两个爬虫内部数据结构（URL 列表、LVS 表）。其中LVS（Label Value Set）表示标签/数值集合，用来表示填充表单的数据源


图3-1-4 Deep Web 爬虫体系结构


Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：
1) 基于领域知识的表单填写：此方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。 Yiyao Lu[25]等人提出一种获取 Form 表单信息的多注解方法，将数据表单按语义分配到各个组中 ，对每组从多方面注解，结合各种注解结果来预测一个最终的注解标签；郑冬冬等人利用一个预定义的领域本体知识库来识别 Deep Web 页面内容， 同时利用一些来自 Web 站点导航模式来识别自动填写表单时所需进行的路径导航 [1]  。
2) 基于网页结构分析的表单填写： 此方法一般无领域知识或仅有有限的领域知识，将网页表单表示成 DOM 树，从中提取表单各字段值。 Desouky 等人提出一种 LEHW 方法，该方法将 HTML 网页表示为DOM 树形式，将表单区分为单属性表单和多属性表单，分别进行处理；孙彬等人提出一种基于 XQuery 的搜索系统，它能够模拟表单和特殊页面标记切换，把网页关键字切换信息描述为三元组单元，按照一定规则排除无效表单，将 Web 文档构造成 DOM 树，利用 XQuery 将文字属性映射到表单字段[1]  。

2.2网络搜索策略
3.2.1广度优先搜索
3.2.2最佳优先搜索
3.2.3深度优先搜索



3.3 爬虫抓取策略
下述的三种网络特征，造成了设计网页爬虫抓取策略变得很难：
1.它巨大的数据量；
2.它快速的更新频率；
3. 动态页面的产生
它们三个特征一起产生了很多种类的爬虫抓取链接
巨大的数据量暗示了爬虫，在给定的时间内，只可以抓取所下载网络的一部分，所以，它需要对它的抓取页面设置优先级；快速的更新频率说明在爬虫抓取下载某网站一个网页的时候，很有可能在这个站点又有新的网页被添加进来，或者这个页面被更新或者删除了。

3.3.1选择策略
决定所要下载的页面；
3.3.2重新访问策略
决定什么时候检查页面的更新变化；
3.3.3 平衡礼貌策略
指出怎样避免站点超载；
3.3.4 并行策略
指出怎么协同达到分布式抓取的效果；



3.4网络爬虫体系结构
3.4.1 体系结构
网页爬虫的高层体系结构一个爬虫不能像上面所说的，仅仅只有一个好的抓取策略，还需要有一个高度优化的结构。Shkapenyuk和Suel（Shkapenyuk和Suel，2002）指出：设计一个短时间内，一秒下载几个页面的颇慢的爬虫是一件很容易的事情，而要设计一个使用几周可以下载百万级页面的高性能的爬虫，将会在系统设计，I/O和网络效率，健壮性和易用性方面遇到众多挑战。
网路爬虫是搜索引擎的核心，他们算法和结构上的细节被当作商业机密。当爬虫的设计发布时，总会有一些为了阻止别人复制工作而缺失的细节。人们也开始关注主要用于阻止主要搜索引擎发布他们的排序算法的“搜索引擎垃圾邮件”。

3.4.2 URL 一般化
爬虫通常会执行几种类型的URL规范化来避免重复抓取某些资源。URL一般化也被称为URL标准化，指的是修正URL并且使其前后一致的过程。这里有几种一般化方法，包括转化URL为小写的，去除逗号（如‘.’ ‘..’等），对非空的路径，在末尾加反斜杠。
3.4.4 身份识别
网络爬虫通过使用http请求的用户代理(User Agent)字段来向网络服务器表明他们的身份。网络管理员则通过检查网络服务器的日志，使用用户代理字段来辨认哪一个爬虫曾经访问过以及它访问的频率。用户代理字段可能会包含一个可以让管理员获取爬虫更多信息的URL。邮件抓取器和其他怀有恶意的网络爬虫通常不会留任何的用户代理字段内容，或者他们也会将他们的身份伪装成浏览器或者其他的知名爬虫。
对于网路爬虫，留下用户标志信息是十分重要的；这样，网络管理员在需要的时候就可以联系爬虫的主人。有时，爬虫可能会陷入爬虫陷阱或者使一个服务器超负荷，这时，爬虫主人需要使爬虫停止。对那些有兴趣了解特定爬虫访问时间网络管理员来讲，用户标识信息是十分重要的。

